{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "root = '../../'\n",
    "sys.path.append(root)   # Done to be able to import the packages and functions\n",
    "\n",
    "import Utils.hico_evaluation.evaluation as ev\n",
    "from Utils.custom_sampler import OverSampler\n",
    "from Utils.custom_loss import MaskedBCELoss\n",
    "from Utils.annotation_preprocessing import _load_csv_to_tensor\n",
    "from Utils.train_val_split import train_val_split_hico\n",
    "from hoi_classifiers import ElementwiseAvgFusionModel\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42   #note that the model parameters will still be randomly initiated\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.load(root + \"Embeddings/Combined_Embeddings/test.pt\")\n",
    "y_test = _load_csv_to_tensor(root + \"anno/added/anno_augmented_test.csv\").T[:,:600] # Transpose to make both first dimensions the #samples.\n",
    "y_test[y_test.isnan()] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = [root + \"../hico_20150920/images/test2015/\" + fname for fname in os.listdir(root + \"../hico_20150920/images/test2015\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_baseline_model = torch.load(root + \"Saved Models/visual_baseline_model.pt\").to(device)\n",
    "bestmodel_67epochs_model = torch.load(root + \"/Saved Models/best_model_67_epochs.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the models for inference on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_visual_baseline = visual_baseline_model(X_test.to(device)[:,0,:])\n",
    "output_bestmodel_67epochs = bestmodel_67epochs_model(X_test.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the number images where all annotations are correctly predicted (ignoring the multitask learning classes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual baseline Accuracy:                   25.440049699730793\n",
      "Elementwise Averaging (67 epochs) Accuracy: 31.186581072685854\n"
     ]
    }
   ],
   "source": [
    "visual_baseline_acc =(torch.where(((output_visual_baseline[:,:600] >= 0.5).int() == (y_test == 1).to(device)).all(dim=1))[0].shape[0] / y_test.to(device).shape[0]) * 100\n",
    "best_model_67epochs_acc =(torch.where(((output_bestmodel_67epochs[:,:600] >= 0.5).int() == (y_test == 1).to(device)).all(dim=1))[0].shape[0] / y_test.to(device).shape[0]) * 100\n",
    "\n",
    "print(f\"Visual baseline Accuracy:                   {visual_baseline_acc}\")\n",
    "print(f\"Elementwise Averaging (67 epochs) Accuracy: {best_model_67epochs_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at which samples are correctly classified by each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_samples_visual_baseline =torch.where(((output_visual_baseline[:,:600] >= 0.5).int() == (y_test == 1).to(device)).all(dim=1))[0]\n",
    "correct_samples_best_model_67epochs =torch.where(((output_bestmodel_67epochs[:,:600] >= 0.5).int() == (y_test == 1).to(device)).all(dim=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corbaseline_in_corbest =torch.isin(correct_samples_visual_baseline, correct_samples_best_model_67epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corbest_in_corbaseline =torch.isin(correct_samples_best_model_67epochs,correct_samples_visual_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples correctly by the baseline but not anymore with the elementwise average fusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids_misses_elementwiseavg =correct_samples_visual_baseline[torch.where(corbaseline_in_corbest == False)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path in [test_paths[i] for i in sample_ids_misses_elementwiseavg]:\n",
    "    #shutil.copy(path, f\"../../../missed_by_elementwiseAvg/{path.split('../../../hico_20150920/images/test2015/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selecting 4 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../hico_20150920/images/test2015/HICO_test2015_00002481.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00006636.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00002526.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00000053.jpg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomindices1 = torch.randint(len(sample_ids_misses_elementwiseavg),(4,))\n",
    "[test_paths[i] for i in sample_ids_misses_elementwiseavg[randomindices1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([385], device='cuda:0'),)\n",
      "(tensor([131, 141], device='cuda:0'),)\n",
      "(tensor([94], device='cuda:0'),)\n",
      "(tensor([467, 470, 472, 478, 479, 480, 481], device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "for i in sample_ids_misses_elementwiseavg[randomindices1]:\n",
    "    print(torch.where((output_bestmodel_67epochs[i,:600] >= 0.5) != (y_test[i] == 1).to(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2427, 6532, 2470,   52], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids_misses_elementwiseavg[randomindices1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_bestmodel_67epochs[52,:600] >= 0.5)[481]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_test == 1)[52][467]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples correctly by the elementwise average fusion but not by baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids_misses_baseline =correct_samples_best_model_67epochs[torch.where(corbest_in_corbaseline == False)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for path in [test_paths[i] for i in sample_ids_misses_baseline]:\n",
    "    #shutil.copy(path, f\"../../../missed_by_baseline/{path.split('../../../hico_20150920/images/test2015/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly selecting 4 images:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../hico_20150920/images/test2015/HICO_test2015_00003349.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00005927.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00006241.jpg',\n",
       " '../../../hico_20150920/images/test2015/HICO_test2015_00008595.jpg']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomindices2 = torch.randint(len(sample_ids_misses_baseline),(4,))\n",
    "[test_paths[i] for i in sample_ids_misses_baseline[randomindices2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([589], device='cuda:0'),)\n",
      "(tensor([370, 371, 374], device='cuda:0'),)\n",
      "(tensor([470, 471, 472, 478], device='cuda:0'),)\n",
      "(tensor([377, 380], device='cuda:0'),)\n"
     ]
    }
   ],
   "source": [
    "for i in sample_ids_misses_baseline[randomindices2]:\n",
    "    print(torch.where((output_visual_baseline[i,:600] >= 0.5) != (y_test[i] == 1).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3275, 5823, 6137, 8489], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids_misses_baseline[randomindices2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_visual_baseline[8489,:600] >= 0.5)[380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([266]),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(y_test[8489] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../hico_20150920/images/test2015/HICO_test2015_00008595.jpg'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_paths[8489]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
